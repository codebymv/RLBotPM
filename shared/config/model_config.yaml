# PPO Model Configuration
# These hyperparameters control how the RL agent learns

ppo:
  # Policy type: MlpPolicy or MlpLstmPolicy
  policy_type: "MlpPolicy"
  # Learning rate - how fast the agent learns
  # 1e-4 for new feature sets (arbitrage mode)
  # 3e-4 is standard for training from scratch
  learning_rate: 0.0001
  
  # Number of steps to collect per environment before updating
  # Higher = more stable but slower learning
  n_steps: 2048
  
  # Mini-batch size for training
  # Must be a divisor of (n_steps * n_envs)
  batch_size: 256
  
  # Number of training epochs per rollout
  # Higher = more learning per data collected, but risk overfitting
  n_epochs: 10
  
  # Discount factor (gamma)
  # How much to value future rewards (0.99 = very forward-looking)
  gamma: 0.99
  
  # GAE lambda for advantage estimation
  # Balances bias vs variance in advantage estimates
  gae_lambda: 0.95
  
  # PPO clipping parameter
  # Limits how much policy can change per update (prevents destructive updates)
  clip_range: 0.2
  
  # Entropy coefficient
  # Encourages exploration (higher = more random actions)
  ent_coef: 0.03
  
  # Value function coefficient
  # Weight of value loss in total loss
  vf_coef: 0.5
  
  # Gradient clipping
  # Prevents exploding gradients
  max_grad_norm: 0.5

recurrent:
  # Only used when policy_type is MlpLstmPolicy
  sequence_length: 1
  lstm_hidden_size: 128
  lstm_layers: 1
  lstm_dropout: 0.0
  shared_lstm: false
  enable_critic_lstm: true

# Training configuration
training:
  # Total training episodes
  total_timesteps: 300000
  
  # Evaluate every N episodes
  eval_frequency: 10000
  
  # Save checkpoint every N episodes
  checkpoint_frequency: 10000
  
  # Number of parallel environments (for faster training)
  # Set to 1 for easier debugging, 4+ for production
  num_parallel_envs: 4
  
  # Use GPU if available
  use_gpu: true
  
  # TensorBoard logging
  tensorboard_log: "./logs/tensorboard"
  
  # Verbosity (0=none, 1=info, 2=debug)
  verbose: 1

  # Early stopping configuration
  early_stopping:
    eval_episodes: 50
    patience: 15
    min_delta: 0.0
    metric: "profit_factor"

# Environment configuration
environment:
  # Initial capital for backtesting
  initial_capital: 1000.0
  
  # Maximum steps per episode
  max_steps: 500
  
  # Transaction cost percentage (0.05% = maker fee level)
  transaction_cost: 0.0005
  
  # Reward function weights
  reward_weights:
    returns: 10.0
    transaction_cost_penalty: 100.0
    drawdown_penalty: 10.0
    profitable_close_bonus: 0.5
    risk_violation_penalty: 1.0
    long_term_profitability: 3.0

# Curriculum learning stages
curriculum:
  enabled: true
  
  stages:
    - name: "Stage 1 - Simple Markets"
      timesteps: 20000
      description: "Highly liquid, simple binary markets"
      filters:
        min_volume: 50000
        min_liquidity: 5000
        categories: ["sports", "entertainment"]
    
    - name: "Stage 2 - Mixed Markets"
      timesteps: 30000
      description: "Medium liquidity, mixed categories"
      filters:
        min_volume: 10000
        min_liquidity: 1000
        categories: ["sports", "entertainment", "crypto"]
    
    - name: "Stage 3 - All Markets"
      timesteps: 50000
      description: "All market types"
      filters:
        min_volume: 1000
        min_liquidity: 500
        categories: null  # All categories
